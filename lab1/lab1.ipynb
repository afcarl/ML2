{
 "metadata": {
  "name": "",
  "signature": "sha256:8ec9e861c3e81c8f8075f5d4fb9b9dfad323b11a43638d7e5d4a3ac13918bf9a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 1: Independent Component Analysis\n",
      "\n",
      "### Machine Learning 2: February 2015\n",
      "\n",
      "* The lab exercises should be made in groups of two people.\n",
      "* The deadline is Sunday, Feb 15, 23:59.\n",
      "* Assignment should be sent to D.P.Kingma at uva dot nl (Durk Kingma). The subject line of your email should be \"[MLPM2014] lab#_lastname1\\_lastname2\". \n",
      "* Put your and your teammates' names in the body of the email\n",
      "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[MLPM2014] lab01\\_Kingma\\_Hu\", the attached file should be \"lab01\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
      "\n",
      "Notes on implementation:\n",
      "\n",
      "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
      "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
      "* NOTE: test your code and make sure we can run your notebook / scripts!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Literature\n",
      "In this assignment, we will implement the Independent Component Analysis algorithm as described in chapter 34 of David MacKay's book \"Information Theory, Inference, and Learning Algorithms\", which is freely available here:\n",
      "http://www.inference.phy.cam.ac.uk/mackay/itila/book.html\n",
      "\n",
      "Read the ICA chapter carefuly before you continue!\n",
      "\n",
      "### Notation\n",
      "\n",
      "$\\mathbf{X}$ is the $M \\times T$ data matrix, containing $M$ measurements at $T$ time steps.\n",
      "\n",
      "$\\mathbf{S}$ is the $S \\times T$ source matrix, containing $S$ source signal values at $T$ time steps. We will assume $S = M$.\n",
      "\n",
      "$\\mathbf{A}$ is the mixing matrix. We have $\\mathbf{X} = \\mathbf{A S}$.\n",
      "\n",
      "$\\mathbf{W}$ is the matrix we aim to learn. It is the inverse of $\\mathbf{A}$, up to indeterminacies (scaling and permutation of sources).\n",
      "\n",
      "$\\phi$ is an elementwise non-linearity or activation function, typically applied to elements of $\\mathbf{W X}$.\n",
      "\n",
      "### Code\n",
      "In the following assignments, you can make use of the signal generators listed below.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "\n",
      "# Signal generators\n",
      "def sawtooth(x, period=0.2, amp=1.0, phase=0.):\n",
      "    return (((x / period - phase - 0.5) % 1) - 0.5) * 2 * amp\n",
      "\n",
      "def sine_wave(x, period=0.2, amp=1.0, phase=0.):\n",
      "    return np.sin((x / period - phase) * 2 * np.pi) * amp\n",
      "\n",
      "def square_wave(x, period=0.2, amp=1.0, phase=0.):\n",
      "    return ((np.floor(2 * x / period - 2 * phase - 1) % 2 == 0).astype(float) - 0.5) * 2 * amp\n",
      "\n",
      "def triangle_wave(x, period=0.2, amp=1.0, phase=0.):\n",
      "    return (sawtooth(x, period, 1., phase) * square_wave(x, period, 1., phase) + 0.5) * 2 * amp\n",
      "\n",
      "def random_nonsingular_matrix(d=2):\n",
      "    \"\"\"\n",
      "    Generates a random nonsingular (invertible) matrix if shape d*d\n",
      "    \"\"\"\n",
      "    epsilon = 0.1\n",
      "    A = np.random.rand(d, d)\n",
      "    while abs(np.linalg.det(A)) < epsilon:\n",
      "        A = np.random.rand(d, d)\n",
      "    return A\n",
      "\n",
      "def plot_signals(X):\n",
      "    \"\"\"\n",
      "    Plot the signals contained in the rows of X.\n",
      "    \"\"\"\n",
      "    figure()\n",
      "    for i in range(X.shape[0]):\n",
      "        ax = plt.subplot(X.shape[0], 1, i + 1)\n",
      "        plot(X[i, :])\n",
      "        ax.set_xticks([])\n",
      "        ax.set_yticks([])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code generates some toy data to work with."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate data\n",
      "num_sources = 5\n",
      "signal_length = 500\n",
      "t = linspace(0, 1, signal_length)\n",
      "S = np.c_[sawtooth(t), sine_wave(t, 0.3), square_wave(t, 0.4), triangle_wave(t, 0.25), np.random.randn(t.size)].T\n",
      "\n",
      "plot_signals(S)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.1 Make mixtures (5 points)\n",
      "Write a function `make_mixtures(S, A)' that takes a matrix of source signals $\\mathbf{S}$ and a mixing matrix $\\mathbf{A}$, and generates mixed signals $\\mathbf{X}$. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_mixtures(S,A):\n",
      "    return A.dot(S)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.2 Histogram (5 points)\n",
      "Write a function `plot_histograms(X)` that takes a data-matrix $\\mathbf{X}$ and plots one histogram for each signal (row) in $\\mathbf{X}$. You can use the numpy `histogram()` function. \n",
      "\n",
      "Plot histograms of the sources and the measurements. \n",
      "Which of these distributions (sources or measurements) tend to look more like Gaussians? Why is this important for ICA? Can you think of an explanation for this phenomenon?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_histograms(X):\n",
      "    R,C = X.shape\n",
      "    for row in xrange(R):\n",
      "        hist, bins = np.histogram(X[row,:])\n",
      "        center = (bins[:-1] + bins[1:]) / 2\n",
      "        plt.subplot(int('1' + str(R) + str(row + 1)))\n",
      "        plt.bar(center, hist)\n",
      "    plt.tight_layout()\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = random_nonsingular_matrix(5)\n",
      "X_toy = make_mixtures(S,A)\n",
      "\n",
      "plot_histograms(X_toy)\n",
      "#Answer questions of 1.2!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.3 Implicit priors (20 points)\n",
      "As explained in MacKay's book, an activation function $\\phi$ used in the ICA learning algorithm corresponds to a prior distribution over sources. Specifically, $\\phi(a) = \\frac{d}{da} \\ln p(a)$. For each of the following activation functions, derive the source distribution they correspond to.\n",
      "$$\\phi_0(a) = -\\tanh(a)$$\n",
      "$$\\phi_1(a) = -a + \\tanh(a)$$\n",
      "$$\\phi_2(a) = -a^3$$\n",
      "$$\\phi_3(a) = -\\frac{6a}{a^2 + 5}$$\n",
      "\n",
      "The normalizing constant is not required, so an answer of the form $p(a) \\propto \\verb+[answer]+$ is ok.\n",
      "\n",
      "Plot the activation functions and the corresponding prior distributions. Compare the shape of the priors to the histogram you plotted in the last question."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\int \\phi_0(a) = -\\ln \\tanh(a)$\n",
      "\n",
      "$p(a) \\propto -cosh(a)$\n",
      "\n",
      "$\\int \\phi_1(a) = -\\frac{1}{2}a^2 +\\ln \\cosh(a)$\n",
      "\n",
      "$p(a) \\propto e^{-\\frac{1}{2}a^2}\\cdot \\cosh(a)$\n",
      "\n",
      "$\\int \\phi_2(a) = -\\frac{1}{4}a^4$\n",
      "\n",
      "$p(a) \\propto e^{-\\frac{1}{4}a^4}$\n",
      "\n",
      "$\\int \\phi_3(a) = -3\\cdot \\ln(a^2+5)$\n",
      "\n",
      "$p(a) \\propto e^3\\cdot (x^2+5)$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.4 Whitening (15 points)\n",
      "Some ICA algorithms can only learn from whitened data. Write a method `whiten(X)` that takes a $M \\times T$ data matrix $\\mathbf{X}$ (where $M$ is the dimensionality and $T$ the number of examples) and returns a whitened matrix. If you forgot what whitening is or how to compute it, various good sources are available online, such as http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def whiten(X):\n",
      "    cov = np.cov(X) #Find covariance matrix\n",
      "    w,v = np.linalg.eigh(cov) #Find eigenvalues (w) and eigenvectors (v)\n",
      "    w = w * np.eye(len(w)) #Create matrix with eigenvalues on diagonal\n",
      "    return np.sqrt(np.linalg.inv(w)).dot(v.T).dot(X) #Whiten, equation 3 pdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.5 Interpret results of whitening (10 points)\n",
      "Make scatter plots of the sources, measurements and whitened measurements. Each axis represents a source/measurement and each time-instance is plotted as a dot in this space. You can use the `np.scatter()` function. Describe what you see.\n",
      "\n",
      "Now compute the covariance matrix of the sources, the measurements and the whitened measurements. You can visualize a covariance matrix using the line of code below. Are the signals independent after whitening?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, axarr = plt.subplots(5, 5)\n",
      "\n",
      "for i in xrange(num_sources):\n",
      "    for j in xrange(num_sources):\n",
      "        axarr[i, j].scatter(S[i,:],S[j,:])\n",
      "        axarr[i,j].axis(\"off\")\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, axarr = plt.subplots(5, 5)\n",
      "\n",
      "for i in xrange(num_sources):\n",
      "    for j in xrange(num_sources):\n",
      "        axarr[i,j].scatter(X_toy[i,:],X_toy[j,:])\n",
      "        axarr[i,j].axis(\"off\")\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "white_X_toy = whiten(X_toy)\n",
      "f, axarr = plt.subplots(5, 5)\n",
      "\n",
      "for i in xrange(num_sources):\n",
      "    for j in xrange(num_sources):\n",
      "        axarr[i,j].scatter(white_X_toy[i,:],white_X_toy[j,:])\n",
      "        axarr[i,j].axis(\"off\")\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, axarr = plt.subplots(1, 3)\n",
      "\n",
      "axarr[0].imshow(np.cov(S), cmap='gray', interpolation='nearest')\n",
      "axarr[1].imshow(np.cov(X_toy), cmap='gray', interpolation='nearest')\n",
      "axarr[2].imshow(np.cov(white_X_toy), cmap='gray', interpolation='nearest')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.6 Covariance (5 points)\n",
      "Explain what a covariant algorithm is. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "According to MacKay, a covariant algorithm gives the same results independent of the units in which quantities are measured. This can be done if the curvature of the objective function is taken into account, which makes it possible to make the algorithm covariant to linear rescaling of its parameters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.7 Independent Component Analysis (25 points)\n",
      "Implement the covariant ICA algorithm as described in MacKay. Write a function `ICA(X, activation_function, learning_rate)`, that returns the demixing matrix $\\mathbf{W}$. The input `activation_function` should accept a function such as `lambda a: -tanh(a)`. Update the gradient in batch mode, averaging the gradients over the whole dataset for each update. Try to make it efficient, i.e. use matrix operations instead of loops where possible (loops are slow in interpreted languages such as python and matlab, whereas matrix operations are internally computed using fast C code)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Algorithm 34.4 from MacKay\n",
      "def ICA(X, activation_function, learning_rate):\n",
      "    W = np.random.normal(0,0.05,(X.shape[0],X.shape[0]))\n",
      "    \n",
      "    i = 0\n",
      "    while i == 0 or (np.linalg.norm(dW) > 200 and i < 500):\n",
      "        i += 1\n",
      "        a = W.dot(X)\n",
      "        z = activation_function(a)\n",
      "        X_prime = W.T.dot(a)\n",
      "        dW = W + z.dot(X_prime.T)\n",
      "        W += learning_rate * dW\n",
      "    print np.linalg.norm(dW)\n",
      "    print i\n",
      "    return W"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.8 Experiments  (5 points)\n",
      "Run ICA on the provided signals using each activation function $\\phi_0, \\ldots, \\phi_3$. Plot the retreived signals for each choice of activation function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W1 = ICA(X_toy, lambda a: -np.tanh(a), 0.001)\n",
      "W2 = ICA(X_toy, lambda a: -a + np.tanh(a), 0.05)\n",
      "W3 = ICA(X_toy, lambda a: -a**3, 0.01)\n",
      "W4 = ICA(X_toy, lambda a: -(6*a/(a**2 + 5)), 0.01)\n",
      "Weights = (W1,W2,W3,W4)\n",
      "\n",
      "for W in Weights:\n",
      "    plot_signals(W.dot(X_toy))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.9 Audio demixing (5 points)\n",
      "The 'cocktail party effect' refers to the ability humans have to attend to one speaker in a noisy room. We will now use ICA to solve a similar but somewhat idealized version of this problem. The code below loads 5 sound files and produces 5 mixed sound files, which are saved to disk so you can listen to them. Use your ICA implementation to de-mix these and reproduce the original source signals. As in the previous exercise, try each of the activation functions and report your results.\n",
      "\n",
      "Keep in mind that this problem is easier than the real cocktail party problem, because in real life there are often more sources than measurements (we have only two ears!), and the number of sources is unknown and variable. Also, mixing is not instantaneous in real life, because the sound from one source arrives at each ear at a different point in time. If you have time left, you can think ways to deal with these issues."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.io.wavfile\n",
      "def save_wav(data, out_file, rate):\n",
      "    scaled = np.int16(data / np.max(np.abs(data)) * 32767)\n",
      "    scipy.io.wavfile.write(out_file, rate, scaled)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load audio sources\n",
      "source_files = ['beet.wav', 'beet9.wav', 'beet92.wav', 'mike.wav', 'street.wav']\n",
      "wav_data = []\n",
      "sample_rate = None\n",
      "for f in source_files:\n",
      "    sr, data = scipy.io.wavfile.read(f, mmap=False)\n",
      "    if sample_rate is None:\n",
      "        sample_rate = sr\n",
      "    else:\n",
      "        assert(sample_rate == sr)\n",
      "    wav_data.append(data[:190000])  # cut off the last part so that all signals have same length\n",
      "\n",
      "# Create source and measurement data\n",
      "S = np.c_[wav_data]\n",
      "plot_signals(S)\n",
      "\n",
      "# Requires your function make_mixtures\n",
      "X_real = make_mixtures(S, random_nonsingular_matrix(S.shape[0]))\n",
      "plot_signals(X_real)\n",
      "# Save mixtures to disk, so you can listen to them in your audio player\n",
      "for i in range(X.shape[0]):\n",
      "   save_wav(X_real[i, :], 'X' + str(i) + '.wav', sample_rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Demix the audio sources    \n",
      "W1 = ICA(X_real, lambda a: -a**3, 1e-17)\n",
      "plot_signals(W1.dot(X_real))\n",
      "# W2 = ICA(X, lambda a: -a + np.tanh(a), 1e-12)\n",
      "# W3 = ICA(X, lambda a: -a**3, 1e-17)\n",
      "# W4 = ICA(X, lambda a: -(6*a/(a**2 + 5)), 0.01)\n",
      "# Weights = (W1,W2,W3,W4)\n",
      "\n",
      "# for W in Weights:\n",
      "#     plot_signals(W.dot(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.10 Excess Kurtosis (20 points)\n",
      "The (excess) kurtosis is a measure of 'peakedness' of a distribution. It is defined as\n",
      "$$\n",
      "\\verb+Kurt+[X] = \\frac{\\mu_4}{\\sigma^4} - 3 = \\frac{\\operatorname{E}[(X-{\\mu})^4]}{(\\operatorname{E}[(X-{\\mu})^2])^2} - 3\n",
      "$$\n",
      "Here, $\\mu_4$ is known as the fourth moment about the mean, and $\\sigma$ is the standard deviation.\n",
      "The '-3' term is introduced so that a Gaussian random variable has 0 excess kurtosis.\n",
      "We will now try to understand the performance of the various activation functions by considering the kurtosis of the corresponding priors, and comparing those to the empirical kurtosis of our data.\n",
      "\n",
      "First, compute analytically the kurtosis of the four priors that you derived from the activation functions before. To do this, you will need the normalizing constant of the distribution, which you can either obtain analytically (good practice!), using computer algebra software (e.g. Sage) or by numerical integration (see scipy.integrate).\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-->Add analytical kurtosis of the four priors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now use the `scipy.stats.kurtosis` function, with the `fisher` option set to `True`, to compute the empirical kurtosis of the dummy signals and the real audio signals. Can you use this data to explain the performance of the various activation functions?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats\n",
      "print scipy.stats.kurtosis(X_toy,axis=1, fisher=True, bias=True)\n",
      "print scipy.stats.kurtosis(X_real,axis=1, fisher=True, bias=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---> Explain with data the performance of activation functions"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}