\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{parskip}

\usepackage{subcaption}


\def\*#1{\boldsymbol{#1}}

\def\ci{\perp\!\!\!\perp}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{ML2 - Homework 4}
\author{By Joost van Amersfoort and Otto Fabius}
\maketitle

\section*{1.}

$Q$ approximates $p(\Theta|D)$, this can be seen from the input to the KL divergence, as given in equation 11 in Bishop99.

\section*{2.}

\begin{align*}
\mathcal{L}(Q) &= \int Q(\*\theta) \ln \frac{P(D, \*\theta)}{Q(\*\theta)}d\*\theta \\
Q(\*\theta) &= \prod_k Q_k(\theta_k)
\end{align*}

Substitute in Q, write out the natural logarithm, split the integral, pull $Q_i(\theta_i)$ out of the product and rewrite as the rest as an expectation:

\begin{align*}
\mathcal{L}(Q) &= \int \prod_k Q_k(\theta_k) \ln \frac{P(D, \*\theta)}{\prod_k Q_k(\theta_k)}d\*\theta \\
&= \int \prod_k Q_k(\theta_k) \left [ \ln P(D, \*\theta) - \sum_k \ln Q_k(\theta_k) \right ] d\*\theta \\
&= \int Q_i(\theta_i) \left [ \int \ln P(D, \*\theta) \prod_{k \neq i} Q_k(\theta_k) d\theta_k \right ] d\theta_i - \int Q_i(\theta_i) \ln Q_i(\theta_i) d\theta_i \\
&= \int Q_i(\theta_i) \langle \ln P(D, \*\theta) \rangle_{k \neq i} d\theta_i - \int Q_i(\theta_i) \ln Q_i(\theta_i) d\theta_i
\end{align*}

Now note that the first term is a negative KLD between $Q_i(\theta_i)$ and the expectation, which means that maximizing this expression means minimizing this KLD. We know that the KLD is minimized when $Q_i(\theta_i)$ equals the expectation:

\begin{align*}
\ln Q_i(\theta_i) & \propto \langle \ln P(D, \*\theta) \rangle_{k \neq i} \\
Q_i(\theta_i) &\propto \exp (\langle \ln P(D, \*\theta) \rangle_{k \neq i}) \\
\end{align*}

Now we still need to normalize this, giving:

\begin{align*}
Q_i(\theta_i) &= \frac{\exp (\langle \ln P(D, \*\theta) \rangle_{k \neq i})}{\int \exp (\langle \ln P(D, \*\theta) \rangle_{k \neq i}) d\theta_i} \\
\end{align*}

\subsection*{3.}

\begin{align*}
\ln(P(X, Z, W, \*\alpha, \*\tau, |\*\mu)) = \ln P(X|Z, W, \*\alpha, \*\tau, |\*\mu)+\ln P(Z)+\ln P(\*\mu) + \ln P(\*\tau) + \ln P(W|\*\alpha)+ \ln P(\*\alpha)= \\ 

\sum_{n=1}^{N} -\frac{1}{2}\ln(|\frac{1}{\*\tau I_d|}-\frac{1}{2}(\*{x_n}-(W \*{z_n}-\*\mu))(\frac{1}{\tau} I_d)(\*{x_n}-(W\*{z_n}-\*\mu)) - \frac{d}{2}\ln(2\pi) \\

+ \sum_{i=1}^N -\frac{1}{2}\ln(1)-\frac{1}{2}\*z_n I_q \*z_n^T - \frac{D_q}{2}\ln(2\pi) \\ 

-\frac{1}{2}\ln (|\*\beta^{-1} I_d|) -\frac{1}{2}\*\mu (\beta I_d)
\*\mu - \frac{D_d}{2}\ln (2\pi) \\

+ c_\tau \ln(d_\tau) - \ln \Gamma(c_\tau)+(c_\tau -1)\ln(\tau)-d_\tau \tau\\ 
+ \sum_{i=1}^{q}\{\frac{d}{2}\ln \frac{\alpha_i}{2\pi} - \frac{1}{2}\alpha_i ||W_i||^2\} \\ 
+\sum_{i=1}^{q} \{ a_\alpha \ln(b_\alpha) - \ln \Gamma(a_\alpha)+(a_\alpha -1)\ln(\alpha)-b_\alpha \alpha \}
\end{align*}

Writing this out and grouping terms where possible, we obtain:

\begin{align*}
\ln p(\mathbf{X}, \mathbf{Z}, \mathbf{W}, \mathbf{\alpha}, \tau, \mathbf{\mu}) = -\frac{1}{2}\ln (\frac{1}{\tau \beta})+(2d+d_q)\ln 2\pi +\sum_{n=1}^{N}\{ (\*{x_n}-(W\*{z_n}+\*\mu))\frac{I_d}{\tau}\*{x_n}-(W\*{z_n}+\*\mu)) + \*{z_n z_n}^T + \frac{\*\mu\*\mu^T}{\beta} \\ \sum_{i=1}^{q}\{\frac{d}{2}\ln \frac{\alpha_i}{2\pi} - \frac{1}{2}\alpha_i ||W||^2 -b_\alpha \alpha_i + (a_\alpha -1)\ln(\alpha_i) + a_\alpha)\ln(b_\alpha) - \ln \Gamma (a_\alpha)\} + (-d_\tau \tau + (c_\tau -1) \ln (\tau) + c_\tau) \ln(d_\tau) -\ln \Gamma(c_\tau)
\end{align*}

We can not use this to assess the convergence of the VB PCA algorithm with this log probability, because we cannot compute the true posterior which is necessary to evaluate the joint. We only have a lowerbound, computed with Q, which approximates the posterior.

\subsection*{4.}

The lowerbound consists of two parts:

\begin{align*}
\mathcal{L}(Q) &= \int Q(\theta) \ln p(D, \theta) d\theta - \int Q(\theta) \ln Q(\theta) d\theta \\
\end{align*}

The first integral can be rewritten as a sum of expectation over the factors of P, the second integral can be rewritten as a sum of entropies of the factors of Q. We first show how the second part simplifies to entropies and then tackle the expectations one by one.

First note that the log of Q can be rewritten as a sum of logs, which we can then split in different integrals:

\begin{align*}
& \int Q(\theta) \ln Q(\theta) d\theta = \int Q(\theta) \sum_i \ln Q(\theta_i) d\theta\\
&= \sum_i \left[ \int \prod_j Q_j(\theta_j) \ln Q_i(\theta_i) d\*Z d\*W d\*\alpha d\*\mu d\tau \right]
\end{align*}

Now we pull this integral apart into integrals over specific parameters. Since for all $j \neq i$ this is just the integral over a probability distribution (which is 1) we are left with:

\begin{align*}
&= \sum_i \left[ \int Q_i(\theta_i) \ln Q_i(\theta_i) d\theta_i \right]
\end{align*}

This is however the definition of entropy which are known for the factors of Q:

\begin{align*}
H(Q(Z)) &= \prod_{n=1}^N \left [\frac{D_{z_n}}{2}  (1 + \ln (2\pi)) + \frac{1}{2} \ln |\*\Sigma_{\*z} | \right ] \\
H(Q(\*W)) &= \prod_{k=1}^d \left [ \frac{D_{\tilde{\*w}_k} }{2} (1 + \ln (2\pi)) + \frac{1}{2} \ln |\*\Sigma_{\*w} | \right ] \\
H(Q(\*\alpha)) &= \prod_{i = 1}^q \left [ \tilde{a}_\alpha - \ln \tilde{b}_{\alpha i} + \ln \Gamma(\tilde{a}_\alpha) + (1 - \tilde{a}_\alpha)\psi(\tilde{a}_\alpha) \right ]\\
H(Q(\*\mu)) &= \frac{D_{\*\mu}}{2}  (1 + \ln (2\pi)) + \frac{1}{2} \ln |\*\Sigma_{\*\mu} | \\
H(Q(\tau)) &= \tilde{a}_\tau - \ln \tilde{b}_{\tau} + \ln \Gamma(\tilde{a}_\tau) + (1 - \tilde{a}_\tau)\psi(\tilde{a}_\tau)
\end{align*}

With D the dimensionality of the variable it's indexed by.\\

For the second part of the lower bound $\int Q(\theta) \ln p(D, \theta) d\theta$
\end{document}
