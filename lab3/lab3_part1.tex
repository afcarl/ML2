\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{parskip}

\usepackage{subcaption}


\def\*#1{\boldsymbol{#1}}

\def\ci{\perp\!\!\!\perp}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{ML2 - Lab 3 part 1}
\author{By Joost van Amersfoort and Otto Fabius}
\maketitle

\section*{1.}

$Q$ approximates the true posterior $p(\Theta|D)$, this can be seen from the input to the KL divergence, as given in equation 11 in Bishop99. The KL is the difference between the true posterior and the approximate so it needs to be added (in log space) to the lower bound in order to obtain the log-likelihood. This follows readily from the equation in the assignment.

\section*{2.}

\begin{align*}
\mathcal{L}(Q) &= \int Q(\*\theta) \ln \frac{P(D, \*\theta)}{Q(\*\theta)}d\*\theta \\
Q(\*\theta) &= \prod_k Q_k(\theta_k)
\end{align*}

Substitute in Q, write out the natural logarithm, split the integral, pull $Q_i(\theta_i)$ out of the product and rewrite as the rest as an expectation:

\begin{align*}
\mathcal{L}(Q) &= \int \prod_k Q_k(\theta_k) \ln \frac{P(D, \*\theta)}{\prod_k Q_k(\theta_k)}d\*\theta \\
&= \int \prod_k Q_k(\theta_k) \left [ \ln P(D, \*\theta) - \sum_k \ln Q_k(\theta_k) \right ] d\*\theta \\
&= \int Q_i(\theta_i) \left [ \int \ln P(D, \*\theta) \prod_{k \neq i} Q_k(\theta_k) d\theta_k \right ] d\theta_i - \int Q_i(\theta_i) \ln Q_i(\theta_i) d\theta_i \\
&= \int Q_i(\theta_i) \langle \ln P(D, \*\theta) \rangle_{k \neq i} d\theta_i - \int Q_i(\theta_i) \ln Q_i(\theta_i) d\theta_i
\end{align*}

Now note that the first term is a negative KLD between $Q_i(\theta_i)$ and the expectation, which means that maximizing this expression means minimizing this KLD. We know that the KLD is minimized when $Q_i(\theta_i)$ equals the expectation:

\begin{align*}
\ln Q_i(\theta_i) & \propto \langle \ln P(D, \*\theta) \rangle_{k \neq i} \\
Q_i(\theta_i) &\propto \exp (\langle \ln P(D, \*\theta) \rangle_{k \neq i}) \\
\end{align*}

Now we still need to normalize this, giving:

\begin{align*}
Q_i(\theta_i) &= \frac{\exp (\langle \ln P(D, \*\theta) \rangle_{k \neq i})}{\int \exp (\langle \ln P(D, \*\theta) \rangle_{k \neq i}) d\theta_i} \\
\end{align*}

\subsection*{3.}

\begin{align*}
\ln(P(X, Z, W, \*\alpha, \*\tau, \*\mu)) = \ln P(X|Z, W, \*\alpha, \*\tau, |\*\mu)+\ln P(Z)+\ln P(\*\mu) + \ln P(\*\tau) + \ln P(W|\*\alpha)+ \ln P(\*\alpha)= \\ 
\sum_{n=1}^{N} -\frac{1}{2}\ln(|\frac{1}{\*\tau I_d|}-\frac{1}{2}(\*{x_n}-(W \*{z_n}-\*\mu))(\frac{1}{\tau} I_d)(\*{x_n}-(W\*{z_n}-\*\mu)) - \frac{d}{2}\ln(2\pi) \\
+ \sum_{i=1}^N -\frac{1}{2}\ln(1)-\frac{1}{2}\*z_n I_q \*z_n^T - \frac{D_q}{2}\ln(2\pi) \\ 
-\frac{1}{2}\ln (|\*\beta^{-1} I_d|) -\frac{1}{2}\*\mu (\beta I_d)
\*\mu - \frac{D_d}{2}\ln (2\pi) \\
+ c_\tau \ln(d_\tau) - \ln \Gamma(c_\tau)+(c_\tau -1)\ln(\tau)-d_\tau \tau\\ 
+ \sum_{i=1}^{q}\{\frac{d}{2}\ln \frac{\alpha_i}{2\pi} - \frac{1}{2}\alpha_i ||W_i||^2\} \\ 
+\sum_{i=1}^{q} \{ a_\alpha \ln(b_\alpha) - \ln \Gamma(a_\alpha)+(a_\alpha -1)\ln(\alpha)-b_\alpha \alpha \}
\end{align*}

Writing this out and grouping terms where possible, we obtain:

\begin{align*}
\ln p(\mathbf{X}, \mathbf{Z}, \mathbf{W}, \mathbf{\alpha}, \tau, \mathbf{\mu}) = -\frac{1}{2}\ln (\frac{1}{\tau \beta})+(2d+d_q)\ln 2\pi +\sum_{n=1}^{N}\{ (\*{x_n}-(W\*{z_n}+\*\mu))\frac{I_d}{\tau}\*{x_n}-(W\*{z_n}+\*\mu)) + \*{z_n z_n}^T + \frac{\*\mu\*\mu^T}{\beta} \\ \sum_{i=1}^{q}\{\frac{d}{2}\ln \frac{\alpha_i}{2\pi} - \frac{1}{2}\alpha_i ||W||^2 -b_\alpha \alpha_i + (a_\alpha -1)\ln(\alpha_i) + a_\alpha)\ln(b_\alpha) - \ln \Gamma (a_\alpha)\} + (-d_\tau \tau + (c_\tau -1) \ln (\tau) + c_\tau) \ln(d_\tau) -\ln \Gamma(c_\tau)
\end{align*}

We can not use this to assess the convergence of the VB PCA algorithm, because we first need to marginalize over all the parameters, or in the VB setting over the approximate posterior. This gives a lowerbound which does assess the convergence of the VB PCA algorithm.

\subsection*{4.}

The lowerbound consists of two parts:

\begin{align*}
\mathcal{L}(Q) &= \int Q(\theta) \ln p(D, \theta) d\theta - \int Q(\theta) \ln Q(\theta) d\theta \\
\end{align*}

The first integral can be rewritten as a sum of expectation over the factors of P, the second integral can be rewritten as a sum of entropies of the factors of Q. We first show how the second part simplifies to entropies and then tackle the expectations one by one.

First note that the log of Q can be rewritten as a sum of logs, which we can then split in different integrals:

\begin{align*}
& \int Q(\theta) \ln Q(\theta) d\theta = \int Q(\theta) \sum_i \ln Q(\theta_i) d\theta\\
&= \sum_i \left[ \int \prod_j Q_j(\theta_j) \ln Q_i(\theta_i) d\*Z d\*W d\*\alpha d\*\mu d\tau \right]
\end{align*}

Now we pull this integral apart into integrals over specific parameters. Since for all $j \neq i$ this is just the integral over a probability distribution (which is 1) we are left with:

\begin{align*}
&= \sum_i \left[ \int Q_i(\theta_i) \ln Q_i(\theta_i) d\theta_i \right]
\end{align*}

This is however the definition of entropy which are known for the factors of Q:

%Change to Z = X and X = T
\begin{align*}
H(Q(Z)) &= \prod_{n=1}^N \left [\frac{D_{z_n}}{2}  (1 + \ln (2\pi)) + \frac{1}{2} \ln |\*\Sigma_{\*z} | \right ] \\
H(Q(\*W)) &= \prod_{k=1}^d \left [ \frac{D_{\tilde{\*w}_k} }{2} (1 + \ln (2\pi)) + \frac{1}{2} \ln |\*\Sigma_{\*w} | \right ] \\
H(Q(\*\alpha)) &= \prod_{i = 1}^q \left [ \tilde{a}_\alpha - \ln \tilde{b}_{\alpha i} + \ln \Gamma(\tilde{a}_\alpha) + (1 - \tilde{a}_\alpha)\psi(\tilde{a}_\alpha) \right ]\\
H(Q(\*\mu)) &= \frac{D_{\*\mu}}{2}  (1 + \ln (2\pi)) + \frac{1}{2} \ln |\*\Sigma_{\*\mu} | \\
H(Q(\tau)) &= \tilde{a}_\tau - \ln \tilde{b}_{\tau} + \ln \Gamma(\tilde{a}_\tau) + (1 - \tilde{a}_\tau)\psi(\tilde{a}_\tau)
\end{align*}

With D the dimensionality of the variable its indexed by.\\

For the first part of the lower bound $\int Q(\theta) \ln p(D, \theta) d\theta$, we split it into 6 integrals (each factor of P). This also the definition of the expectation. For each integral we apply the same trick with factors of Q integrating to 1. We also split the log each time to create simpler integrals.

We will now describe the integral for each part of $p(D, \theta)$ separately:

\begin{align*}
 \langle \ln p(\*\mu) \rangle_{Q(\theta)} &= \int Q(\*\mu) \ln p(\*\mu) d\*\mu \\
 &= \int Q(\*\mu)(\ln (2\pi^{\frac{-D_{\*\mu}}{2}} |\beta^{-1}I|^{-\frac{1}{2}}) + (\frac12 \*\mu^T(\beta^{-1}I)^{-1}\*\mu )) d\*\mu 
\end{align*}

Now the first part is independent of $\*\mu$ and the second part can be solved using Matrix cookbook equation 378:

\begin{align*}
&= \ln (2\pi^{\frac{-D_{\*\mu}}{2}} |\beta^{-1}I|^{-\frac{1}{2}}) - \frac12  \mathbb{E}[\*\mu^T(\beta I)\*\mu] \\
&= \ln (2\pi^{\frac{-D_{\*\mu}}{2}} |\beta^{-1}I|^{-\frac{1}{2}}) - \frac12 (\mbox{Tr}(\beta I \*\Sigma_{\*\mu}) + m_\mu^T(\beta I \*\Sigma_{\*\mu})m_\mu)
\end{align*}

For $p(z_n)$ we perform exactly the same steps, except that we pull the sum out of the integral. This time we omit the steps were we split $\ln p(z_n)$ and the other stuff we described above:

\begin{align*}
\langle \ln p(Z) \rangle_{Q(\theta)} &= \sum_{n=1}{N} ( \ln(2\pi^{\frac{D_{z_n}}{2}}) - \frac12 (\mbox{Tr}(\*\Sigma_z) + m_z^T m_z)
\end{align*}

For $p(\*\alpha)$ the situation is a bit different, because we have a Gamma distribution. After performing our usual tricks we end up at:
\begin{align*}
\langle \ln p(\*\alpha) \rangle_{Q(\theta)} &= \sum_{i=1}^q \int Q(\alpha_i) (\ln (\frac{b_\alpha^{a_\alpha}}{\Gamma(a_\alpha)} + (a_\alpha - 1) \ln \alpha_i)  - b_\alpha \alpha_i) d\alpha_i
\end{align*}

Now the first term does not depend on $\alpha_i$, the second term can be done with the hint from the lab $\langle \ln x\rangle = - \ln b + \psi(a)$ and the third term is just the expectation times a constant:

Obtaining in the end:

\begin{align*}
&= \sum_{i=1}^q \left [ \ln \frac{b_\alpha^{a_\alpha}}{\Gamma(a_\alpha)} + (a_\alpha - 1)(- \ln b_\alpha + \psi(a_\alpha)) - b_\alpha \frac{a_\alpha}{b_\alpha} \right] \\
&= \sum_{i=1}^q \left [\ln \frac{b_\alpha^{a_\alpha}}{\Gamma(a_\alpha)} + (a_\alpha - 1)(- \ln b_\alpha + \psi(a_\alpha)) - a_\alpha \right ]
\end{align*}

For $p(\tau)$ the derivation is the same except for the sum. Thus we present the final answer directly:

\begin{align*}
\langle \ln p(\tau) \rangle_{Q(\theta)} &= \ln \frac{\tilde{b}_\tau^{\tilde{a}_\tau}}{\Gamma(\tilde{a}_\tau)} + (\tilde{a}_\tau - 1)(- \ln \tilde{b}_\tau + \psi(\tilde{a}_\tau)) - \tilde{a}_\tau \\
\end{align*}

For $p(\*w|\*\alpha)$ we have to do a bit more work, because $Q(\*\alpha)$ and $Q(\*W)$ remain in the integral:

\begin{align*}
\langle \ln p(\*w|\*\alpha) \rangle_{Q(\theta)} &= \sum_{i = 1}^q \int Q(\*\alpha_i)Q(\*W_i)(\ln \frac{\alpha_i}{2\pi}^{d/2} + (- \frac12 \*\alpha_i ||\*W_i||^2 )) d\*W d\*\alpha
\end{align*}

We split this integral into two parts, then the first part is again split into several parts obtaining:

$$= \frac{d}{2}( - \ln \tilde{b}_{\alpha i} + \psi(\tilde{a}_{\alpha_i}) - \ln 2\pi)$$

For the second part we first push in the integral over $\alpha_i$ then we obtain a value that we can take out of the integral and we are left with an expectation that can be solved by equation 378 from the Matrix Cookbook:

\begin{align*}
& - \frac12 \int Q(\*\alpha_i) Q(\*W_i) \alpha_i ||\*w_i||^2 dw_i d\alpha_i \\
&= - \frac12 \frac{\tilde{a}_\alpha}{\tilde{b}_{\alpha i}} \int Q(\*W_i) ||\*w_i||^2 dw_i \\
&= - \frac12 \frac{\tilde{a}_\alpha}{\tilde{b}_{\alpha i}} (\mbox{Tr}(\*{\Sigma_w}) + (m_w^{(i)})^T m_w^{(i)} )
\end{align*}

The end result is then:

\begin{align*}
\sum_{i = 1}^q \left [ \frac{d}{2}( - \ln \tilde{b}_{\alpha i} + \psi(\tilde{a}_{\alpha_i}) - \ln 2\pi) - \frac12 \frac{\tilde{a}_\alpha}{\tilde{b}_{\alpha i}} (\mbox{Tr}(\*{\Sigma_w}) + (m_w^{(i)})^T m_w^{(i)} ) \right ]
\end{align*}

Now for $p(\*x_n|\*z_n,W,\*\mu,\tau)$:

$$\int \int \int \int Q(\*z_n)Q(\*W_i)Q(\*\mu)Q(\tau)\ln p(\*x_n|\*z_n,W,\*\mu,\tau)d\tau d\*\mu d W d \*z$$
$$= -\frac{D}{2}\ln (2\pi) + \sum_{i=1}^{N}\int \int \int \int Q(\*z_n)Q(\*W)Q(\*\mu )Q(\tau ) D_d \ln (\frac{1}{\tau}) + (\*x_n -(W\*z_n+\*\mu))(\tau I_d)(\*x_n - W\*z_n \*\mu)^T d\tau d\*\mu dW d\*z$$

Splitting the integral into two parts and integrating out the variables that don't occur in the argument of the first quadruple integral:

$$= -\frac{D}{2}\ln (2\pi) + \sum_{i=1}^{N} \{ \int Q(\tau) D_d \ln (\frac{1}{\tau})d\tau + \int \int \int \int Q(\*z_n)Q(\*W)Q(\*\mu )Q(\tau ) (\*x_n -(W\*z_n+\*\mu))(\tau I_d)(\*x_n - W\*z_n \*\mu)^T d\tau d\*\mu dW d\*z \}$$

We then work out the brackets, this yields 9 integrals, most of which depend on a few parameters and are relatively easy to solve. 

The final result (i.e. the actual lower bound) is the sum of all these expectations minus the sum of all the entropies.







\end{document}
