\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{parskip}

\usepackage{subcaption}


\def\*#1{\boldsymbol{#1}}

\def\ci{\perp\!\!\!\perp}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{ML2 - Homework 4}
\author{By Joost van Amersfoort and Otto Fabius}
\maketitle

\section*{1.}

$Q$ approximates $p(\Theta|D)$, this can be seen from the input to the KL divergence, as given in equation 11 in Bishop99.

\section*{2.}

\begin{align*}
\mathcal{L}(Q) &= \int Q(\*\theta) \ln \frac{P(D, \*\theta)}{Q(\*\theta)}d\*\theta \\
Q(\*\theta) &= \prod_k Q_k(\theta_k)
\end{align*}

Substitute in Q, write out the natural logarithm, split the integral, pull $Q_i(\theta_i)$ out of the product and rewrite as the rest as an expectation:

\begin{align*}
\mathcal{L}(Q) &= \int \prod_k Q_k(\theta_k) \ln \frac{P(D, \*\theta)}{\prod_k Q_k(\theta_k)}d\*\theta \\
&= \int \prod_k Q_k(\theta_k) \left [ \ln P(D, \*\theta) - \sum_k \ln Q_k(\theta_k) \right ] d\*\theta \\
&= \int Q_i(\theta_i) \left [ \int \ln P(D, \*\theta) \prod_{k \neq i} Q_k(\theta_k) d\theta_k \right ] d\theta_i - \int Q_i(\theta_i) \ln Q_i(\theta_i) d\theta_i \\
&= \int Q_i(\theta_i) \langle \ln P(D, \*\theta) \rangle_{k \neq i} d\theta_i - \int Q_i(\theta_i) \ln Q_i(\theta_i) d\theta_i
\end{align*}

Now note that the first term is a negative KLD between $Q_i(\theta_i)$ and the expectation, which means that maximizing this expression means minimizing this KLD. We know that the KLD is minimized when $Q_i(\theta_i)$ equals the expectation:

\begin{align*}
\ln Q_i(\theta_i) & \propto \langle \ln P(D, \*\theta) \rangle_{k \neq i} \\
Q_i(\theta_i) &\propto \exp (\langle \ln P(D, \*\theta) \rangle_{k \neq i}) \\
\end{align*}

Now we still need to normalize this, giving:

\begin{align*}
Q_i(\theta_i) &= \frac{\exp (\langle \ln P(D, \*\theta) \rangle_{k \neq i})}{\int \exp (\langle \ln P(D, \*\theta) \rangle_{k \neq i}) d\theta_i} \\
\end{align*}

\subsection*{3.}

\begin{align*}
\ln(P(X, Z, W, \*\alpha, \*\tau, |\*\mu)) = \ln P(X|Z, W, \*\alpha, \*\tau, |\*\mu)+\ln P(Z)+\ln P(\*\mu) + \ln P(\*\tau) + \ln P(W|\*\alpha)+ \ln P(\*\alpha)= \\ 
 -\frac{1}{2}\ln(|\frac{1}{\*\tau I_d|}-\frac{1}{2}(X-(WZ-\*\mu))(\frac{1}{\tau} I_d)(X-(WZ-\*\mu)) - \frac{d}{2}\ln(2\pi) \\
-\frac{1}{2}\ln(1)-\frac{1}{2}ZI_qZ - \frac{d_q}{2}\ln(2\pi) \\ 
-\frac{1}{2}\ln (|\*\beta^{-1} I_d|) -\frac{1}{2}\*\mu I_d
\*\mu - \frac{d}{2}\ln (2\pi) \\
+\ln \Gamma(\tau|c_\tau, d_\tau) \\ 
+ \sum_{i=1}^{q}\{\frac{d}{2}\ln \frac{\alpha_i}{2\pi} - \frac{1}{2}\alpha_i ||W_i||^2\} \\ 
\sum_{i=1}^{q} \ln \gamma (\alpha_i | a_\alpha, b_\alpha )
\end{align*}
Writing this out and grouping terms where possible, we obtain:

\begin{align*}
\ln p(\mathbf{X}, \mathbf{Z}, \mathbf{W}, \mathbf{\alpha}, \tau, \mathbf{\mu}) = -\frac{1}{2}\{\ln (\frac{1}{\tau \beta})+ (X-(WZ-\*\mu))\frac{I_d}{\tau}(X-(WZ+\*\mu)) + ZZ^T + \frac{\*\mu\*\mu^T}{\beta} + (2d+d_q)\ln 2\pi \} \\ \sum_{i=1}^{q}\{\frac{d}{2}\ln \frac{\alpha_i}{2\pi} - \frac{1}{2}\alpha_i ||W||^2 -b_\alpha \alpha_i + (a_\alpha -1)\ln(\alpha_i) + a_\alpha)\ln(b_\alpha) - \ln \Gamma (a_\alpha)\} + (-d_\tau \tau + (c_\tau -1) \ln (\tau) + c_\tau) \ln(d_\tau) -\ln \Gamma(c_\tau)
\end{align*}

We cannot use this to assess the convergence of the VB PCA algorithm with this log probability, because we cannot compute the true posterior which is necessary to evaluate the joint. We only have a lowerbound, computed with Q, which approximates the posterior.



\end{document}
