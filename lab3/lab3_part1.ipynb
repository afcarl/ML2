{
 "metadata": {
  "name": "",
  "signature": "sha256:905d8d0761c84241bf052d480186675f9fb99117fd692797b8820ae3b9d06737"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 3: Bayesian PCA (Part 1)\n",
      "\n",
      "### Machine Learning: Principles and Methods, March 2015\n",
      "\n",
      "* The lab exercises should be made in groups of two people.\n",
      "* The deadline for part 1 is Sunday, 22 March, 23:59.\n",
      "* Assignment should be sent to D.P.Kingma at uva dot nl (Durk Kingma). The subject line of your email should be \"[MLPM2015] lab3part1_lastname1\\_lastname2\". \n",
      "* Put your and your teammates' names in the body of the email\n",
      "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[MLPM2013] lab01\\_Kingma\\_Hu\", the attached file should be \"lab0\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
      "\n",
      "Notes on implementation:\n",
      "\n",
      "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
      "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
      "* NOTE: test your code and make sure we can run your notebook / scripts!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction\n",
      "\n",
      "In this lab assignment, we will implement a variational algorithm for Bayesian PCA. Unlike regular PCA based on maximization of retained variance or minimization of projection error (see Bishop, 12.1.1 and 12.1.2), probabilistic PCA defines a proper density model over observed and latent variables. We will work with a fully Bayesian model this time, which is to say that we will put priors on our parameters and will be interested in learning the posterior over those parameters. Bayesian methods are very elegant, but require a shift in mindset: we are no longer looking for a point estimate of the parameters (as in maximum likelihood or MAP), but for a full posterior distribution.\n",
      "\n",
      "The integrals involved in a Bayesian analysis are usually analytically intractable, so that we must resort to approximations. In this lab assignment, we will implement the variational method described in Bishop99. Chapters 10 and 12 of the PRML book contain additional material that may be useful when doing this exercise.\n",
      "\n",
      "* [Bishop99] Variational Principal Components, C. Bishop, ICANN 1999 - http://research.microsoft.com/pubs/67241/bishop-vpca-icann-99.pdf\n",
      "\n",
      "Below, you will find some code to get you started."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.special as sp\n",
      "\n",
      "class BayesianPCA(object):\n",
      "    \n",
      "    def __init__(self, d, N, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        self.d = d # number of dimensions\n",
      "        self.N = N # number of data points\n",
      "        \n",
      "        # Hyperparameters\n",
      "        self.a_alpha = a_alpha\n",
      "        self.b_alpha = b_alpha\n",
      "        self.a_tau = a_tau\n",
      "        self.b_tau = b_tau\n",
      "        self.beta = beta\n",
      "\n",
      "        # Variational parameters\n",
      "        self.means_z = np.random.randn(d, N) # called x in bishop99\n",
      "        self.sigma_z = np.random.randn(d, d)\n",
      "        self.mean_mu = np.random.randn(d, 1)\n",
      "        self.sigma_mu = np.random.randn(d, d)\n",
      "        self.means_w = np.random.randn(d, d)\n",
      "        self.sigma_w = np.random.randn(d, d)\n",
      "        self.a_alpha_tilde = np.abs(np.random.randn(1))\n",
      "        self.bs_alpha_tilde = np.abs(np.random.randn(d, 1))\n",
      "        self.a_tau_tilde = np.abs(np.random.randn(1))\n",
      "        self.b_tau_tilde = np.abs(np.random.randn(1))\n",
      "    \n",
      "    def __update_z(self, X):\n",
      "        pass\n",
      "    \n",
      "    def __update_mu(self):\n",
      "        pass\n",
      "    \n",
      "    def __update_w(self, X):\n",
      "        pass\n",
      "    \n",
      "    def __update_alpha(self):\n",
      "        pass\n",
      "\n",
      "    def __update_tau(self, X):\n",
      "        pass\n",
      "\n",
      "    def L(self, X):\n",
      "        L = 0.0\n",
      "        expected_tau = self.a_tau_tilde / self.b_tau_tilde\n",
      "        expected_W = self.means_w # (5,)\n",
      "        expected_Zn = self.means_z # (d,N)\n",
      "        expected_mu = self.mean_mu #(d,1)\n",
      "        \n",
      "        expected_WW = self.means_w.T.dot(self.means_w) + np.trace(self.sigma_w)\n",
      "        expected_ZZ = lambda n: self.sigma_z + self.means_z[:,0,np.newaxis].dot(self.means_z[:,0,np.newaxis].T)\n",
      "        expected_mumu = self.mean_mu.T.dot(self.mean_mu) + np.trace(self.sigma_mu)\n",
      "\n",
      "                \n",
      "        #Note to future Otto and Joost: think about expected_W and expected_Z sizes (and which axis to prod over)\n",
      "        L_part = np.zeros((self.N,1))\n",
      "        for i in range(self.N):\n",
      "            L_part[i] = - expected_tau / 2 * (X[i,:].T.dot(X[i,:]) \\\n",
      "                                              - 2*X[i,:,np.newaxis].T.dot(expected_W).dot(expected_Zn[:,i,np.newaxis]) \\\n",
      "                                              - 2*X[i,:,np.newaxis].T.dot(expected_mu) \\\n",
      "                                              + np.trace(expected_WW.dot(expected_ZZ(i)))  \\\n",
      "                                              + 2 * expected_mu.T.dot(expected_W).dot(expected_Zn[:,i,np.newaxis]) \\\n",
      "                                              + expected_mumu)\n",
      "                    \n",
      "                    \n",
      "                    \n",
      "                    \n",
      "                    \n",
      "                    \n",
      "        L += np.sum(L_part)\n",
      "        \n",
      "        L += - self.N * self.d / 2 * np.log(2 * np.pi) + self.N * self.d / 2 * (sp.psi(self.a_tau_tilde) - np.log(self.b_tau_tilde))\n",
      "        \n",
      "        L += - 0.5 * np.sum(np.linalg.norm(self.means_z, axis = 0) ** 2)\n",
      "        \n",
      "        L += - 0.5 * self. N * np.trace(self.sigma_z) - self.N * self.d * np.log(2 * np.pi)\n",
      "        \n",
      "        L += - self.beta / 2 * (np.linalg.norm(self.mean_mu)**2 + np.trace(self.sigma_mu))\n",
      "        \n",
      "        L += - self.d / 2 * np.log(2 * np.pi) + self.d / 2 * np.log(self.beta)\n",
      "        \n",
      "        L += self.a_tau * np.log(self.b_tau) - np.log(sp.gamma(self.a_tau)) + (self.a_tau - 1)*(sp.psi(self.a_tau_tilde) - np.log(self.b_tau_tilde)) - \\\n",
      "        self.b_tau * self.a_tau_tilde / self.b_tau_tilde\n",
      "        \n",
      "        #Axis of norm here might be wrong\n",
      "        L += np.sum(- self.a_alpha_tilde / (2 * self.b_tau_tilde) * (np.linalg.norm(self.means_w, axis= 1)**2 + np.trace(self.sigma_w)) + \\\n",
      "                    self.d / 2 * (sp.psi(self.a_alpha_tilde) - np.log(self.bs_alpha_tilde) - self.d / 2 * np.log(2 * np.pi)))\n",
      "        \n",
      "        L += np.sum(self.a_alpha * np.log(self.b_alpha) + (self.a_alpha - 1) * (sp.psi(self.a_alpha_tilde) - np.log(self.bs_alpha_tilde)) - \\\n",
      "                    self.b_alpha * self.a_alpha_tilde / self.bs_alpha_tilde - np.log(sp.gamma(self.a_alpha)))\n",
      "        \n",
      "        #####ENTROPY TIME#####\n",
      "        entropy_normal = lambda sigma: self.d / 2 * (1 + np.log(2 * np.pi)) + 0.5 * np.log(np.linalg.det(sigma))\n",
      "        \n",
      "        L += entropy_normal(self.sigma_z) * self.N\n",
      "        \n",
      "        L += entropy_normal(self.sigma_mu)\n",
      "        \n",
      "        L += entropy_normal(self.sigma_w) * self.d\n",
      "        \n",
      "        entropy_gamma = lambda alpha, beta: alpha - np.log(beta) + np.log(sp.gamma(alpha)) + (1 - alpha)*sp.psi(alpha)\n",
      "        \n",
      "        #unsure about sum here\n",
      "        L += np.sum(entropy_gamma(self.a_alpha_tilde, self.bs_alpha_tilde))\n",
      "        L += entropy_gamma(self.a_tau_tilde, self.b_tau_tilde)\n",
      "        \n",
      "        return L\n",
      "    \n",
      "    def fit(self, X):\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = 5\n",
      "N = 10\n",
      "BayesianPCA = BayesianPCA(d, N)\n",
      "X = np.random.normal(0,1,(N,d))\n",
      "\n",
      "print BayesianPCA.L(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4. The lower bound $\\mathcal{L}(Q)$ (25 points)\n",
      "\n",
      "Derive an expression for the lower bound $\\mathcal{L}(Q)$ of the log-prob $\\ln p(X)$ for Bayesian PCA, making use of the factorization (eq. 12) and the form of the Q-distributions (eq. 16-20) as listed in [Bishop99]. Show your steps. Implement this function."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5. Optimize variational parameters (50 points)\n",
      "Implement the update equations for the Q-distributions, in the __update_XXX methods. Each update function should re-estimate the variational parameters of the Q-distribution corresponding to one group of variables (i.e. either $Z$, $\\mu$, $W$, $\\alpha$ or $\\tau$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6. Learning algorithm (10 points)\n",
      "Implement the learning algorithm described in [Bishop99], i.e. iteratively optimize each of the Q-distributions holding the others fixed.\n",
      "\n",
      "What would be a good way to track convergence of the algorithm? Implement your suggestion.\n",
      "\n",
      "Test the algorithm on some test data drawn from a Gaussian with different variances in orthogonal directions. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7. PCA Representation of MNIST (10 points)\n",
      "\n",
      "Download the MNIST dataset from here http://deeplearning.net/tutorial/gettingstarted.html (the page contains python code for loading the data). Run your algorithm on (part of) this dataset, and visualize the results."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}